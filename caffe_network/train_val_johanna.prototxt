name: "CaffeNet"
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "label_depth"
  top: "label_normal"
  include { phase: TRAIN }

  hdf5_data_param {
    source: "/home/rendus/CSG/out_test/CSG_test_dataset.txt"
    batch_size: 16
  }
}

layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "label_depth"
  top: "label_normal"
  include { phase: TEST }

  hdf5_data_param {
    source: "/home/rendus/CSG/out_test/CSG_test_dataset.txt"
    batch_size: 16
  }
}




# *****************
# ***** conv1 *****
# *****************
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    #stride: 2
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "conv1_2norm"
  type: "BatchNorm"
  bottom: "conv1_2"
  top: "conv1_2norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv2 *****
# *****************
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv1_2norm"
  top: "conv2_1"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 3
    dilation: 2
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 3
    #stride: 2
    dilation: 2
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "conv2_2norm"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv3 *****
# *****************
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv2_2norm"
  top: "conv3_1"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 256
    pad: 4
    kernel_size: 3
    dilation: 4
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 256
    pad: 4
    kernel_size: 3
    dilation: 4
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "conv3_2"
  top: "conv3_3"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 256
    pad: 4
    kernel_size: 3
    #stride: 2
    dilation: 4
  }
}
layer {
  name: "relu3_3"
  type: "ReLU"
  bottom: "conv3_3"
  top: "conv3_3"
}
layer {
  name: "conv3_3norm"
  type: "BatchNorm"
  bottom: "conv3_3"
  top: "conv3_3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv4 *****
# *****************
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv3_3norm"
  top: "conv4_1"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 8
    dilation: 8
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 8
    dilation: 8
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "conv4_2"
  top: "conv4_3"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 8
    dilation: 8
  }
}
layer {
  name: "relu4_3"
  type: "ReLU"
  bottom: "conv4_3"
  top: "conv4_3"
}
layer {
  name: "conv4_3norm"
  type: "BatchNorm"
  bottom: "conv4_3"
  top: "conv4_3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv5 *****
# *****************
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "conv4_3norm"
  top: "conv5_1"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE  
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 16
    dilation: 16
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 16
    dilation: 16
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "conv5_3"
  type: "Convolution"
  bottom: "conv5_2"
  top: "conv5_3"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 512
    kernel_size: 3
    stride: 1
    pad: 16
    dilation: 16
  }
}
layer {
  name: "relu5_3"
  type: "ReLU"
  bottom: "conv5_3"
  top: "conv5_3"
}
layer {
  name: "conv5_3norm"
  type: "BatchNorm"
  bottom: "conv5_3"
  top: "conv5_3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}


# # ****************************
# # ***** Unary prediction *****
# # ****************************

layer {
  name: "conv_s2_5_depth"
  type: "Convolution"
  bottom: "conv_5_3norm"
  top: "conv_s2_5_depth"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 51
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}


layer {
  name: "loss_depth"
  type: "SigmoidCrossEntropyLoss"
  bottom: "conv_s2_5_depth"
  bottom: "label_depth"
  top: "loss_depth"
  loss_param {
    normalize: false;
  }
}


layer {
  name: "max_label_depth"
  type: "ArgMax"
  bottom: "label_depth"
  top: "max_label_depth"
  argmax_param {
      axis : 1;
      }
  include { phase: TEST }

}


layer {
  name: "accu_depth"
  type: "Accuracy"
  bottom: "conv_s2_5_depth"
  bottom: "max_label_depth"
  top: "accu_depth"
  accuracy_param {
    ignore_label : 50;
  }
  include { phase: TEST }
}



layer {
  name: "accu_depth2"
  type: "Accuracy"
  bottom: "conv_s2_5_depth"
  bottom: "max_label_depth"
  top: "accu_depth2"
  include { phase: TEST }
}


layer {
  name: "conv_s2_5_normal"
  type: "Convolution"
  bottom: "conv_5_3"
  top: "conv_s2_5_normal"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 51
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}


layer {
  name: "loss_normal"
  type: "SigmoidCrossEntropyLoss"
  bottom: "conv_s2_5_normal"
  bottom: "label_normal"
  top: "loss_normal"
  loss_param {
    normalize: false;
  }
}


layer {
  name: "max_label_normal"
  type: "ArgMax"
  bottom: "label_normal"
  top: "max_label_normal"
  argmax_param {
      axis : 1;
      }
  include { phase: TEST }

}


layer {
  name: "accu_normal"
  type: "Accuracy"
  bottom: "conv_s2_5_normal"
  bottom: "max_label_normal"
  top: "accu_normal"
  accuracy_param {
    ignore_label : 50;
  }
  include { phase: TEST }
}



layer {
  name: "accu_normal2"
  type: "Accuracy"
  bottom: "conv_s2_5_normal"
  bottom: "max_label_normal"
  top: "accu_normal2"
  include { phase: TEST }
}

